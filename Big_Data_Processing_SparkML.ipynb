{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Drankenstain/Data_Analytics/blob/main/Big_Data_Processing_SparkML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rHZHkHVcw8x"
      },
      "source": [
        "!pip install -q findspark\n",
        "!pip install -q pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBAYqaXOc0y6"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector"
      ],
      "metadata": {
        "id": "BRaSH27DjPXc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZD9_VSVzEgI"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "denseVector = Vectors.dense([1.0, 0.0, 0.0, -2.0])\n",
        "sparseVector = Vectors.sparse(4, [(0, 1.0), (3, -2.0)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "denseVector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjn9CILDHLU7",
        "outputId": "c7bd8092-1769-48f6-c82f-5e95ebf2db44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseVector([1.0, 0.0, 0.0, -2.0])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sparseVector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB-Mzds9Qkma",
        "outputId": "99f1dd3a-d83c-4770-db2d-767e68ab9079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseVector(4, {0: 1.0, 3: -2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Statistics"
      ],
      "metadata": {
        "id": "56YIS5bsjTxW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zppxBKyAc3B5",
        "outputId": "0c1688a3-dae7-4f2c-9f9d-cdde5ff728d6"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
        "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
        "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
        "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
        "df = spark.createDataFrame(data, [\"features\"])\n",
        "\n",
        "r1 = Correlation.corr(df, \"features\").head()\n",
        "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
        "\n",
        "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
        "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson correlation matrix:\n",
            "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
            "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
            "             [       nan,        nan, 1.        ,        nan],\n",
            "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
            "Spearman correlation matrix:\n",
            "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
            "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
            "             [       nan,        nan, 1.        ,        nan],\n",
            "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcpT11YmdA3q",
        "outputId": "a0b2bbb8-a09c-40c2-a839-761183f5d101"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import ChiSquareTest\n",
        "\n",
        "data = [(1.0, Vectors.dense(0.5, 10.0)),\n",
        "        (2.0, Vectors.dense(1.5, 20.0)),\n",
        "        (3.0, Vectors.dense(1.5, 30.0)),\n",
        "        (3.0, Vectors.dense(3.5, 30.0)),\n",
        "        (4.0, Vectors.dense(3.5, 40.0)),\n",
        "        (4.0, Vectors.dense(3.5, 40.0))]\n",
        "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
        "\n",
        "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
        "print(\"pValues: \" + str(r.pValues))\n",
        "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
        "print(\"statistics: \" + str(r.statistics))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pValues: [0.14734918392814822,0.03517353946698465]\n",
            "degreesOfFreedom: [6, 9]\n",
            "statistics: [9.500000000000002,18.000000000000007]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.stat import Summarizer\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "df = spark.sparkContext.parallelize([Row(weight=0.1, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
        "                     Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()"
      ],
      "metadata": {
        "id": "lAj6X-jbHK2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = Summarizer.metrics(\"mean\", \"count\")\n",
        "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDFKhjLOHY3z",
        "outputId": "76206aa2-783a-47e6-937e-515f28562411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+\n",
            "|aggregate_metrics(features, weight)|\n",
            "+-----------------------------------+\n",
            "|{[1.0,1.0,1.0], 1}                 |\n",
            "+-----------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qofz5EYMdGU6",
        "outputId": "83c41607-50cf-4d62-ba08-23e6b1da48dd"
      },
      "source": [
        "from pyspark.ml.stat import Summarizer\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "df = spark.sparkContext.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
        "                     Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
        "\n",
        "# create summarizer for multiple metrics \"mean\" and \"count\"\n",
        "summarizer = Summarizer.metrics(\"mean\", \"count\")\n",
        "\n",
        "# compute statistics for multiple metrics with weight\n",
        "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)\n",
        "\n",
        "# compute statistics for multiple metrics without weight\n",
        "df.select(summarizer.summary(df.features)).show(truncate=False)\n",
        "\n",
        "# compute statistics for single metric \"mean\" with weight\n",
        "df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)\n",
        "\n",
        "# compute statistics for single metric \"mean\" without weight\n",
        "df.select(Summarizer.mean(df.features)).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+\n",
            "|aggregate_metrics(features, weight)|\n",
            "+-----------------------------------+\n",
            "|{[1.0,1.0,1.0], 1}                 |\n",
            "+-----------------------------------+\n",
            "\n",
            "+--------------------------------+\n",
            "|aggregate_metrics(features, 1.0)|\n",
            "+--------------------------------+\n",
            "|{[1.0,1.5,2.0], 2}              |\n",
            "+--------------------------------+\n",
            "\n",
            "+--------------+\n",
            "|mean(features)|\n",
            "+--------------+\n",
            "|[1.0,1.0,1.0] |\n",
            "+--------------+\n",
            "\n",
            "+--------------+\n",
            "|mean(features)|\n",
            "+--------------+\n",
            "|[1.0,1.5,2.0] |\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binomial Logistic Regression"
      ],
      "metadata": {
        "id": "ngyaRHatjs6N"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgehAeLResBu",
        "outputId": "c06823a6-4fc3-478e-f895-c9597e96de30"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-04 05:33:02--  https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 104736 (102K) [text/plain]\n",
            "Saving to: ‘sample_libsvm_data.txt’\n",
            "\n",
            "\rsample_libsvm_data.   0%[                    ]       0  --.-KB/s               \rsample_libsvm_data. 100%[===================>] 102.28K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-05-04 05:33:03 (7.07 MB/s) - ‘sample_libsvm_data.txt’ saved [104736/104736]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Load training data\n",
        "training = spark.read.format(\"libsvm\").load(\"sample_libsvm_data.txt\")"
      ],
      "metadata": {
        "id": "Jqv_Z4Snxprq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMRQ3WqAJNiw",
        "outputId": "4be22b1b-ecbf-4a71-d33c-4a9395c99278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(label=0.0, features=SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0}))]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh4CPinoeVay",
        "outputId": "a2b9908b-bf9a-477f-a135-5cf2ea46e37f"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Load training data\n",
        "training = spark.read.format(\"libsvm\").load(\"sample_libsvm_data.txt\")\n",
        "\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "\n",
        "# Fit the model\n",
        "lrModel = lr.fit(training)\n",
        "\n",
        "# Print the coefficients and intercept for logistic regression\n",
        "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
        "print(\"Intercept: \" + str(lrModel.intercept))\n",
        "\n",
        "# We can also use the multinomial family for binary classification\n",
        "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
        "\n",
        "# Fit the model\n",
        "mlrModel = mlr.fit(training)\n",
        "\n",
        "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
        "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
        "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: (692,[272,300,323,350,351,378,379,405,406,407,428,433,434,435,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.520689871384125e-05,-8.115773146847006e-05,3.814692771846427e-05,0.0003776490540424338,0.0003405148366194403,0.0005514455157343107,0.0004085386116096912,0.0004197467332749452,0.0008119171358670031,0.000502770837266875,-2.3929260406600902e-05,0.0005745048020902297,0.0009037546426803677,7.818229700243899e-05,-2.1787551952911914e-05,-3.402165821789542e-05,0.0004966517360637633,0.0008190557828370372,-8.017982139522613e-05,-2.743169403783527e-05,0.0004810832226238988,0.0004840801762677878,-8.926472920009901e-06,-0.00034148812330427297,-8.950592574121382e-05,0.00048645469116892156,-8.478698005186097e-05,-0.00042347832158317705,-7.296535777631246e-05])\n",
            "Intercept: -0.5991460286401438\n",
            "Multinomial coefficients: 2 X 692 CSRMatrix\n",
            "(0,272) 0.0001\n",
            "(0,300) 0.0001\n",
            "(0,350) -0.0002\n",
            "(0,351) -0.0001\n",
            "(0,378) -0.0003\n",
            "(0,379) -0.0002\n",
            "(0,405) -0.0002\n",
            "(0,406) -0.0004\n",
            "(0,407) -0.0002\n",
            "(0,433) -0.0003\n",
            "(0,434) -0.0005\n",
            "(0,435) -0.0001\n",
            "(0,456) 0.0\n",
            "(0,461) -0.0002\n",
            "(0,462) -0.0004\n",
            "(0,483) 0.0001\n",
            "..\n",
            "..\n",
            "Multinomial intercepts: [0.27505875857180895,-0.27505875857180895]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht_akl1PedwS",
        "outputId": "427fdbfa-5254-40a5-b6e4-fcbe1f1b4e89"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
        "# in the earlier example\n",
        "trainingSummary = lrModel.summary\n",
        "\n",
        "# Obtain the objective per iteration\n",
        "objectiveHistory = trainingSummary.objectiveHistory\n",
        "print(\"objectiveHistory:\")\n",
        "for objective in objectiveHistory:\n",
        "    print(objective)\n",
        "\n",
        "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
        "trainingSummary.roc.show()\n",
        "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
        "\n",
        "# Set the model threshold to maximize F-Measure\n",
        "fMeasure = trainingSummary.fMeasureByThreshold\n",
        "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
        "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
        "    .select('threshold').head()['threshold']\n",
        "lr.setThreshold(bestThreshold)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "objectiveHistory:\n",
            "0.6833149135741672\n",
            "0.6661906127558117\n",
            "0.6207433672479603\n",
            "0.6131541253123869\n",
            "0.6059149689952393\n",
            "0.5923656241678249\n",
            "0.589823308283802\n",
            "0.5868012627420282\n",
            "0.5844432058719142\n",
            "0.5830790068041746\n",
            "0.5807015754032354\n",
            "+---+--------------------+\n",
            "|FPR|                 TPR|\n",
            "+---+--------------------+\n",
            "|0.0|                 0.0|\n",
            "|0.0|0.017543859649122806|\n",
            "|0.0| 0.03508771929824561|\n",
            "|0.0| 0.05263157894736842|\n",
            "|0.0| 0.07017543859649122|\n",
            "|0.0| 0.08771929824561403|\n",
            "|0.0| 0.10526315789473684|\n",
            "|0.0| 0.12280701754385964|\n",
            "|0.0| 0.14035087719298245|\n",
            "|0.0| 0.15789473684210525|\n",
            "|0.0| 0.17543859649122806|\n",
            "|0.0| 0.19298245614035087|\n",
            "|0.0| 0.21052631578947367|\n",
            "|0.0| 0.22807017543859648|\n",
            "|0.0| 0.24561403508771928|\n",
            "|0.0|  0.2631578947368421|\n",
            "|0.0|  0.2807017543859649|\n",
            "|0.0|  0.2982456140350877|\n",
            "|0.0|  0.3157894736842105|\n",
            "|0.0|  0.3333333333333333|\n",
            "+---+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "areaUnderROC: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression_4e06baac1579"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multinomial Logistic Regression"
      ],
      "metadata": {
        "id": "q_Qb2F0ljnv_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZHOR1HlfV97",
        "outputId": "27cf22a9-6968-4f83-8328-2de4d7d75671"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_multiclass_classification_data.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-03 17:38:11--  https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_multiclass_classification_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6953 (6.8K) [text/plain]\n",
            "Saving to: ‘sample_multiclass_classification_data.txt’\n",
            "\n",
            "\r          sample_mu   0%[                    ]       0  --.-KB/s               \rsample_multiclass_c 100%[===================>]   6.79K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-05-03 17:38:12 (34.3 MB/s) - ‘sample_multiclass_classification_data.txt’ saved [6953/6953]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SglmBZNOfARS",
        "outputId": "bd7ff879-7ac1-433c-bf99-fa20c270d222"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Load training data\n",
        "training = spark \\\n",
        "    .read \\\n",
        "    .format(\"libsvm\") \\\n",
        "    .load(\"sample_multiclass_classification_data.txt\")\n",
        "\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "\n",
        "# Fit the model\n",
        "lrModel = lr.fit(training)\n",
        "\n",
        "# Print the coefficients and intercept for multinomial logistic regression\n",
        "print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\n",
        "print(\"Intercept: \" + str(lrModel.interceptVector))\n",
        "\n",
        "trainingSummary = lrModel.summary\n",
        "\n",
        "# Obtain the objective per iteration\n",
        "objectiveHistory = trainingSummary.objectiveHistory\n",
        "print(\"objectiveHistory:\")\n",
        "for objective in objectiveHistory:\n",
        "    print(objective)\n",
        "\n",
        "# for multiclass, we can inspect metrics on a per-label basis\n",
        "print(\"False positive rate by label:\")\n",
        "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
        "    print(\"label %d: %s\" % (i, rate))\n",
        "\n",
        "print(\"True positive rate by label:\")\n",
        "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
        "    print(\"label %d: %s\" % (i, rate))\n",
        "\n",
        "print(\"Precision by label:\")\n",
        "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
        "    print(\"label %d: %s\" % (i, prec))\n",
        "\n",
        "print(\"Recall by label:\")\n",
        "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
        "    print(\"label %d: %s\" % (i, rec))\n",
        "\n",
        "print(\"F-measure by label:\")\n",
        "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
        "    print(\"label %d: %s\" % (i, f))\n",
        "\n",
        "accuracy = trainingSummary.accuracy\n",
        "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
        "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
        "fMeasure = trainingSummary.weightedFMeasure()\n",
        "precision = trainingSummary.weightedPrecision\n",
        "recall = trainingSummary.weightedRecall\n",
        "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
        "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: \n",
            "3 X 4 CSRMatrix\n",
            "(0,3) 0.305\n",
            "(1,2) -0.7666\n",
            "(1,3) -0.3854\n",
            "Intercept: [0.05192580020728831,-0.12619173083598803,0.07426593062869971]\n",
            "objectiveHistory:\n",
            "1.098612288668108\n",
            "1.0767872580801818\n",
            "1.0324898663050683\n",
            "1.0276148685544233\n",
            "1.0292979666409194\n",
            "1.0238563624020458\n",
            "1.0236260835897755\n",
            "1.0235478802964153\n",
            "1.0231925082158748\n",
            "1.0231565244620064\n",
            "1.0229939986213705\n",
            "False positive rate by label:\n",
            "label 0: 0.22\n",
            "label 1: 0.05\n",
            "label 2: 0.0\n",
            "True positive rate by label:\n",
            "label 0: 1.0\n",
            "label 1: 1.0\n",
            "label 2: 0.46\n",
            "Precision by label:\n",
            "label 0: 0.6944444444444444\n",
            "label 1: 0.9090909090909091\n",
            "label 2: 1.0\n",
            "Recall by label:\n",
            "label 0: 1.0\n",
            "label 1: 1.0\n",
            "label 2: 0.46\n",
            "F-measure by label:\n",
            "label 0: 0.819672131147541\n",
            "label 1: 0.9523809523809523\n",
            "label 2: 0.6301369863013699\n",
            "Accuracy: 0.82\n",
            "FPR: 0.09\n",
            "TPR: 0.82\n",
            "F-measure: 0.800730023276621\n",
            "Precision: 0.8678451178451179\n",
            "Recall: 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "4UaPFEvxjxAt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD3dgaYef0WK",
        "outputId": "18d8de2c-c7c1-4504-a90f-fdcd7424bf82"
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Load training data\n",
        "data = spark.read.format(\"libsvm\") \\\n",
        "    .load(\"sample_libsvm_data.txt\")\n",
        "\n",
        "# Split the data into train and test\n",
        "splits = data.randomSplit([0.6, 0.4], 1234)\n",
        "train = splits[0]\n",
        "test = splits[1]\n",
        "\n",
        "# create the trainer and set its parameters\n",
        "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
        "\n",
        "# train the model\n",
        "model = nb.fit(train)\n",
        "\n",
        "# select example rows to display.\n",
        "predictions = model.transform(test)\n",
        "predictions.show()\n",
        "\n",
        "# compute accuracy on the test set\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
        "                                              metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test set accuracy = \" + str(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+-----------+----------+\n",
            "|label|            features|       rawPrediction|probability|prediction|\n",
            "+-----+--------------------+--------------------+-----------+----------+\n",
            "|  0.0|(692,[95,96,97,12...|[-172664.79564650...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[98,99,100,1...|[-176279.15054306...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[122,123,124...|[-189600.55409526...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[124,125,126...|[-274673.88337431...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[124,125,126...|[-183393.03869049...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[125,126,127...|[-256992.48807619...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[126,127,128...|[-210411.53649773...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[127,128,129...|[-170627.63616681...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[127,128,129...|[-212157.96750469...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[127,128,129...|[-183253.80108550...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[128,129,130...|[-246528.93739632...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[150,151,152...|[-158348.34683571...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[152,153,154...|[-210229.50765957...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[152,153,154...|[-242985.16248889...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[152,153,154...|[-94622.933454005...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[153,154,155...|[-266465.39689814...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[153,154,155...|[-144989.71469229...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[154,155,156...|[-283834.57437738...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[181,182,183...|[-155256.59399829...|  [1.0,0.0]|       0.0|\n",
            "|  1.0|(692,[100,101,102...|[-147726.11958982...|  [0.0,1.0]|       1.0|\n",
            "+-----+--------------------+--------------------+-----------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Test set accuracy = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "b0f1kFNHj1md"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8Kp7V2uhAwr",
        "outputId": "0821cf3a-0797-4f36-9152-0672468ae023"
      },
      "source": [
        "!wget https://github.com/apache/spark/raw/master/data/mllib/sample_linear_regression_data.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-03 17:38:17--  https://github.com/apache/spark/raw/master/data/mllib/sample_linear_regression_data.txt\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_linear_regression_data.txt [following]\n",
            "--2024-05-03 17:38:17--  https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_linear_regression_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 119069 (116K) [text/plain]\n",
            "Saving to: ‘sample_linear_regression_data.txt’\n",
            "\n",
            "sample_linear_regre 100%[===================>] 116.28K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2024-05-03 17:38:17 (12.9 MB/s) - ‘sample_linear_regression_data.txt’ saved [119069/119069]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_GPqfH5glZj",
        "outputId": "59a3fbed-25f0-4ba6-ab89-72bd2bdfc9a6"
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "# Load training data\n",
        "training = spark.read.format(\"libsvm\")\\\n",
        "    .load(\"sample_linear_regression_data.txt\")\n",
        "\n",
        "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "\n",
        "# Fit the model\n",
        "lrModel = lr.fit(training)\n",
        "\n",
        "# Print the coefficients and intercept for linear regression\n",
        "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
        "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
        "\n",
        "# Summarize the model over the training set and print out some metrics\n",
        "trainingSummary = lrModel.summary\n",
        "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
        "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
        "trainingSummary.residuals.show()\n",
        "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
        "print(\"r2: %f\" % trainingSummary.r2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [0.0,0.3229251667740594,-0.3438548034562219,1.915601702345841,0.05288058680386255,0.765962720459771,0.0,-0.15105392669186676,-0.21587930360904645,0.2202536918881343]\n",
            "Intercept: 0.15989368442397356\n",
            "numIterations: 6\n",
            "objectiveHistory: [0.49999999999999994, 0.4967620357443381, 0.49363616643404634, 0.4936351537897608, 0.4936351214177871, 0.49363512062528014, 0.4936351206216114]\n",
            "+--------------------+\n",
            "|           residuals|\n",
            "+--------------------+\n",
            "|  -9.889232683103197|\n",
            "|  0.5533794340053553|\n",
            "|  -5.204019455758822|\n",
            "| -20.566686715507508|\n",
            "|    -9.4497405180564|\n",
            "|  -6.909112502719487|\n",
            "|  -10.00431602969873|\n",
            "|  2.0623978070504845|\n",
            "|  3.1117508432954772|\n",
            "|  -15.89360822941938|\n",
            "|  -5.036284254673026|\n",
            "|  6.4832158769943335|\n",
            "|  12.429497299109002|\n",
            "|  -20.32003219007654|\n",
            "|    -2.0049838218725|\n",
            "| -17.867901734183793|\n",
            "|   7.646455887420495|\n",
            "| -2.2653482182417406|\n",
            "|-0.10308920436195645|\n",
            "|  -1.380034070385301|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "RMSE: 10.189077\n",
            "r2: 0.022861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Regression"
      ],
      "metadata": {
        "id": "TqxYStlPj48x"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWjpeZJihCvz",
        "outputId": "680b5198-8890-4fce-9c29-a5b7e68c1c4a"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.feature import VectorIndexer\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Load the data stored in LIBSVM format as a DataFrame.\n",
        "data = spark.read.format(\"libsvm\").load(\"sample_libsvm_data.txt\")\n",
        "\n",
        "# Automatically identify categorical features, and index them.\n",
        "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "featureIndexer =\\\n",
        "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a DecisionTree model.\n",
        "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
        "\n",
        "# Chain indexer and tree in a Pipeline\n",
        "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
        "\n",
        "# Train model.  This also runs the indexer.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
        "\n",
        "treeModel = model.stages[1]\n",
        "# summary only\n",
        "print(treeModel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+--------------------+\n",
            "|prediction|label|            features|\n",
            "+----------+-----+--------------------+\n",
            "|       0.0|  0.0|(692,[121,122,123...|\n",
            "|       0.0|  0.0|(692,[122,123,124...|\n",
            "|       0.0|  0.0|(692,[123,124,125...|\n",
            "|       0.0|  0.0|(692,[124,125,126...|\n",
            "|       0.0|  0.0|(692,[124,125,126...|\n",
            "+----------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Root Mean Squared Error (RMSE) on test data = 0\n",
            "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_276ae3f2fb05, depth=2, numNodes=5, numFeatures=692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means Clustering"
      ],
      "metadata": {
        "id": "N0xXuUrRj-9Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3pIJ8fujyrS",
        "outputId": "a3b24720-10e0-49e1-c857-0c966b69749e"
      },
      "source": [
        "import numpy as np\n",
        "from pyspark.mllib.clustering import KMeans\n",
        "\n",
        "!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
        "\n",
        "def parseVector(line):\n",
        "  return np.array([float(x) for x in line.split(',')])\n",
        "\n",
        "lines = spark.sparkContext.textFile('wine.data')\n",
        "data = lines.map(parseVector)\n",
        "k=3\n",
        "model = KMeans.train(data, k)\n",
        "print(\"Final centers: \"+str(model.clusterCenters))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final centers: [array([2.27536232e+00, 1.25166667e+01, 2.49420290e+00, 2.28855072e+00,\n",
            "       2.08231884e+01, 9.23478261e+01, 2.07072464e+00, 1.75840580e+00,\n",
            "       3.90144928e-01, 1.45188406e+00, 4.08695651e+00, 9.41159420e-01,\n",
            "       2.49072464e+00, 4.58231884e+02]), array([1.02127660e+00, 1.38044681e+01, 1.88340426e+00, 2.42617021e+00,\n",
            "       1.70234043e+01, 1.05510638e+02, 2.86723404e+00, 3.01425532e+00,\n",
            "       2.85319149e-01, 1.91042553e+00, 5.70255319e+00, 1.07829787e+00,\n",
            "       3.11404255e+00, 1.19514894e+03]), array([2.25806452e+00, 1.29298387e+01, 2.50403226e+00, 2.40806452e+00,\n",
            "       1.98903226e+01, 1.03596774e+02, 2.11112903e+00, 1.58403226e+00,\n",
            "       3.88387097e-01, 1.50338710e+00, 5.65032258e+00, 8.83967742e-01,\n",
            "       2.36548387e+00, 7.28338710e+02])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uL92E00izzXY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}